# -*- coding: utf-8 -*-
"""mlba_ass2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bKaWOGqKoN1yO_sEdpXtEGijVRM-ezaf
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/content/gdrive')
# %cd '/content/gdrive/MyDrive/ML datasets'
# %ls

import warnings
warnings.filterwarnings('ignore')

#importing libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV
from collections import Counter
from sklearn.ensemble import RandomForestRegressor
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from keras.preprocessing.sequence import pad_sequences

#loading the training dataset
data=pd.read_csv('train.csv')

#loading the testing dataset
data_test=pd.read_csv('test.csv')

#storing IDs from testing dataset
test_data_id= []
for id,seq in zip(data_test['ID'], data_test[' Sequence']):
    test_data_id.append(id)

#Null values detection
print("Number of null values:")
print(data.isnull().sum())
#Train data is balanced
print("Checking data imbalance:")
print(Counter(data[' Label']))
print("\n\n")

#Analysis of length of each sequence (to find max_length)
import statistics

#Analysis of length of each sequence in train dataset
a=[]
for seq in data['Sequence']:
  a.append(len(seq))
print(Counter(a))
print(statistics.mean(a))

#Analysis of length of each sequence in test dataset
b=[]
for seq in data_test[' Sequence']:
  b.append(len(seq))
print(Counter(b))
print(statistics.mean(b))
print("\n\n")

#Pre-Processing

#Integer Encoding
char_dict= {'A': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6, 'H': 7, 'I': 8, 'K': 9, 'L': 10, 'M': 11, 'N': 12, 'P': 13, 'Q': 14, 'R': 15, 'S': 16, 'T': 17, 'V': 18, 'W': 19, 'X':20, 'Y': 21}
def feature_extraction(data, column):
  encode_list = []
  for row in data[column].values:
    row_encode = []
    for code in row:
      row_encode.append(char_dict.get(code, 0))
    encode_list.append(np.array(row_encode))
  #Sequence Padding 
  encode_list= pad_sequences(encode_list, maxlen=25, padding='post', truncating='post')
  encode_list= feature_extraction_2(encode_list)
  return encode_list

#Feature Extraction: Binary Profiling, Frequency count, Molecular Weight
molecular_weight_dict={1:89.1,15:174.2,12:132.1,3:133.1,2:121.2,4:147.1,14:146.2,6:75.1,7:155.2,8:131.2,10:131.2,9:146.2,11:149.2,5:165.2,13:115.1,16:105.1,17:119.1,19:204.2,21:181.2,18:117.1}
def feature_extraction_2(e_list):
  encode_list = []
  for row in e_list:
    row_encode = []
    temp_array=[]
    weight=0
    for d in list(range(1, 22)):
      temp_array.append((row == d).sum())
    for code in row:
      gene_string=[0 for k in range(21)]
      gene_string[code-1]=1
      temp_array.extend(gene_string)
      weight+=molecular_weight_dict.get(code, 0)
    row_encode.append(weight)
    row_encode.extend(temp_array)
    encode_list.append(np.array(row_encode))
  return encode_list

#Features and Labels splitting
X_data = feature_extraction(data, 'Sequence')
Y_data = data[' Label'].values
X_data_test = feature_extraction(data_test, ' Sequence')

#Splitting data into train and validation dataset to check accuracy
X_train, X_val, Y_train, Y_val = train_test_split(X_data, Y_data, test_size=0.2, random_state=123,stratify= Y_data)

"""#Best Model:"""

#Best model: RandomForestRegressor

print("Best model: RandomForestRegressor: \n")
#min_samples_split=10 and n_estimators=100 to avoid overfitting and underfitting
rf = RandomForestRegressor(n_estimators=100, n_jobs=-1) 

#GridSearchCV to apply  K-fold cross Validation and Hypeparameter tuning
grid = GridSearchCV(estimator = rf, param_grid = {}, cv = 3, verbose=2, n_jobs = -1)
grid.fit(X_data, Y_data)

#Best estimator from GridSearchCV
print(grid.best_estimator_)
forest= grid.best_estimator_

#Predicting values
predictions = forest.predict(X_data_test)

#Converting output into csv file
output_data= []
for i in range(len(predictions)):
    output_data.append([test_data_id[i],predictions[i]])
output_df=pd.DataFrame(output_data,columns=['ID','Label'])
output_df.to_csv('output_RFR.csv', sep=',', index=False)
print("\n Output File: \n")
print(output_df)
print("\n\n\n")

"""#Other Models:"""

#Best model:  RandomForestClassifier

print("Model:  RandomForestClassifier \n")
#min_samples_split=10 and n_estimators=100 to avoid overfitting and underfitting
rc = RandomForestClassifier(n_estimators=100, n_jobs=-1) 

#GridSearchCV to apply  K-fold cross Validation and Hypeparameter tuning
grid = GridSearchCV(estimator = rc, param_grid = {}, cv = 3, verbose=2, n_jobs = -1)
grid.fit(X_train, Y_train)

#Best estimator from GridSearchCV
print(grid.best_estimator_)
forestc= grid.best_estimator_

#Chceking accuracy on val data
Y_pred_val= forestc.predict(X_val)
print("\n Classification Report for Validation dataset: \n")
print(classification_report(Y_val, Y_pred_val))


#Predicting values
predictions = forestc.predict(X_data_test)

#Converting output into csv file
output_data= []
for i in range(len(predictions)):
    output_data.append([test_data_id[i],predictions[i]])
output_df=pd.DataFrame(output_data,columns=['ID','Label'])
output_df.to_csv('output_RFC.csv', sep=',', index=False)
print("\n Output File: \n")
print(output_df)
print("\n\n\n")

#Model: Logistic Regression

print("Model: Logistic Regression: \n")

log = LogisticRegression(n_jobs=-1) 

#GridSearchCV to apply  K-fold cross Validation and Hypeparameter tuning
grid = GridSearchCV(estimator = log, param_grid = {}, cv = 10, verbose=2, n_jobs = -1)
grid.fit(X_train, Y_train)

#Best estimator from GridSearchCV
print(grid.best_estimator_)
logistic= grid.best_estimator_

#Chceking accuracy on val data
Y_pred_val= logistic.predict(X_val)
print("\n Classification Report for Validation dataset: \n")
print(classification_report(Y_val, Y_pred_val))


#Predicting values
predictions = logistic.predict(X_data_test)

#Converting output into csv file
output_data= []
for i in range(len(predictions)):
    output_data.append([test_data_id[i],predictions[i]])
output_df=pd.DataFrame(output_data,columns=['ID','Label'])
output_df.to_csv('output_Logistic.csv', sep=',', index=False)
print("\n Output File: \n")
print(output_df)
print("\n\n\n")

#Model: SVM

print("Model: SVM: \n")
svm = make_pipeline(StandardScaler(), SVC(gamma='auto'))

#training the model
svm.fit(X_train, Y_train)

#Chceking accuracy on val data
Y_pred_val= svm.predict(X_val)
print("\n Classification Report for Validation dataset: \n")
print(classification_report(Y_val, Y_pred_val))

#Predicting values
predictions = svm.predict(X_data_test)

#Converting output into csv file
output_data= []
for i in range(len(predictions)):
    output_data.append([test_data_id[i],int(predictions[i])])
output_df=pd.DataFrame(output_data,columns=['ID','Label'])
output_df.to_csv('output_SVM.csv', sep=',', index=False)
print("\n Output File: \n")
print(output_df)
print("\n\n\n")

#Model: Kth Nearest Neighbour

print("Model: KNN \n")
knn = KNeighborsClassifier(n_neighbors=20)

#training the model
knn.fit(X_train, Y_train)

#Chceking accuracy on val data
Y_pred_val= knn.predict(X_val)
print("\n Classification Report for Validation dataset: \n")
print(classification_report(Y_val, Y_pred_val))

#Predicting values
predictions = knn.predict(X_data_test)

#Converting output into csv file
output_data= []
for i in range(len(predictions)):
    output_data.append([test_data_id[i],int(predictions[i])])
output_df=pd.DataFrame(output_data,columns=['ID','Label'])
output_df.to_csv('output_KNN.csv', sep=',', index=False)
print("\n Output File: \n")
print(output_df)
print("\n\n\n")